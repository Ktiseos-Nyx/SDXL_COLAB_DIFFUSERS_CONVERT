{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **SDXL Model Converter Colab Edition**\n",
        "\n",
        "**Be aware**\n",
        "\n",
        "Gemini made most of the refactored code in this, and i'm still working on cleaning some content up. It MAY or MAY NOT work, but I don't have the time to fullly test it asap. Please make an note of it in the repository if there is an issue or feature request: https://github.com/Ktiseos-Nyx/SDXL_COLAB_DIFFUSERS_CONVERT\n",
        "\n",
        "---\n",
        "## **⚠️IMPORTANT: Google Colab AUP Warning & Xformers Disclaimer⚠️**\n",
        "---\n",
        " **⚠️ WARNING:**\n",
        "This Google Colab notebook includes tools from the `kohya-ss/sd-scripts` repository, which is **primarily designed for training Stable Diffusion models**. Using this code may cause your Google Colab account to be flagged for violating the Google Colab Acceptable Use Policy (AUP), **even if you are not actively training a model**.\n",
        "\n",
        "**⚠️ IMPORTANT AUP RISK:**\n",
        " Using this notebook carries a **significant risk of AUP violations** which may lead to the suspension of your Google Colab account and it is provided **AS-IS**\n",
        "\n",
        " **⚠️ XFORMERS WARNING:**\n",
        " The `xformers` library may not be compatible with all hardware configurations or versions of PyTorch, and it may lead to errors. The use of `xformers` and the reliability of this code is entirely at your own risk.\n",
        "\n",
        " **⚠️AUP RISK⚠️**\n",
        " While it's not explicitly stated within the AUP, nor terms of service, slowdowns and over use on the free plans could lead to account strikes or removal. Please be aware that Ktiseos Nyx, Duskfallcrew, and even Google are not at fault for what pranks you try and pull with the AUP.\n",
        "\n",
        " Be patient. Be kind. Rewind.\n",
        "\n",
        " I'm working on a re-do of the gradio!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Links\n",
        "\n",
        "| Link Name| Description | Link |\n",
        "| --- | --- | --- |\n",
        "| Huggingface Backup| backup checkpoints! | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://github.com/Ktiseos-Nyx/HuggingFace_Backup)\n",
        "|Discord| E&D Discord |[Invite](https://discord.gg/5t2kYxt7An)\n",
        "|Huggingface| E&D Huggingface |[Earth & Dusk](https://huggingface.co/EarthnDusk)\n",
        "|Ko-Fi| Kofi Support |[![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/Z8Z8L4EO)\n",
        "|Github| Ktiseos Nyx |[Ktiseos Nyx](https://github.com/Ktiseos-Nyx/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## **Important Note:**\n",
        "\n",
        "Before diving in, ensure you create a Hugging Face token with write permissions. Follow this link for instructions on token creation.\n",
        "\n",
        "You need to create a huggingface token, go to [this link](https://huggingface.co/settings/tokens), then `create new token` or copy available token with the `Write` role.\n",
        "\n",
        "\n",
        "### **Setup Instructions:**\n",
        "\n",
        "1.  **Google Colab Environment:** This notebook is designed to run on Google Colab with a GPU runtime enabled.\n",
        "2.  **Root Directory:** The `root_dir` variable specifies where files will be stored. The default is `/content`. You can specify a different location by changing the value in the corresponding text field.\n",
        "3. **Repository URL:** If you want to use a specific version of the `kohya-ss/sd-scripts` repository, then you can specify the URL using the text field for \"Repo URL\".\n",
        "4. **Branch Name:** If you want to use a specific branch of the `kohya-ss/sd-scripts` repository, then you can specify the branch name using the text field for \"Branch\".\n",
        "5. **Xformers URL:** You are using the official xformers library, however, if you have a custom built `xformers` wheel file, then you can specify the URL using the \"Xformers URL\" field. (Not Recommended).\n",
        "6.  **Run Setup:** After providing the values, click the \"Setup Environment\" button to install all required dependencies and clone the necessary files, and prepare the environment variables.\n",
        "7.  **Follow the Steps:** You are now able to continue with the following steps.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IlZmggT-dlpO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kMXVrsFAdSKi"
      },
      "outputs": [],
      "source": [
        "# @title ## **Install Kohya Script**\n",
        "import os\n",
        "from subprocess import getoutput\n",
        "from ipywidgets import Text, Output, Button\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Widget creation (for the UI)\n",
        "root_dir_widget = Text(value=\"/content\", placeholder=\"Root Directory\", description=\"Root Directory:\")\n",
        "repo_url_widget = Text(value=\"https://github.com/kohya-ss/sd-scripts\", placeholder=\"Repo URL\", description=\"Repo URL:\")\n",
        "branch_widget = Text(value=\"sdxl\", placeholder=\"Branch\", description=\"Branch:\")\n",
        "output_widget = Output()\n",
        "setup_button = Button(description=\"Setup Environment\")\n",
        "\n",
        "\n",
        "def setup_environment(root_dir, repo_url, branch, output_widget):\n",
        "    \"\"\"Sets up the environment: creates directories, clones repo, installs dependencies.\"\"\"\n",
        "    try:\n",
        "        repo_dir, models_dir, tools_dir, vae_dir = create_directories(root_dir)\n",
        "        clone_repo(repo_url, repo_dir, branch, output_widget)\n",
        "        install_dependencies(output_widget)\n",
        "        prepare_environment()\n",
        "\n",
        "        with output_widget:\n",
        "            print(f\"Root directory set to: {root_dir}\")\n",
        "            print(\"Setup Complete\")\n",
        "\n",
        "    except Exception as e:\n",
        "        with output_widget:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "\n",
        "def create_directories(root_dir):\n",
        "    \"\"\"Creates the necessary directories.\"\"\"\n",
        "    repo_dir = os.path.join(root_dir, \"kohya-trainer\")\n",
        "    models_dir = os.path.join(root_dir, \"models\")\n",
        "    tools_dir = os.path.join(repo_dir, \"tools\")\n",
        "    vae_dir = os.path.join(root_dir, \"vae\")\n",
        "\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "    os.makedirs(vae_dir, exist_ok=True)\n",
        "    os.makedirs(tools_dir, exist_ok=True)\n",
        "    os.makedirs(repo_dir, exist_ok=True)\n",
        "\n",
        "    return repo_dir, models_dir, tools_dir, vae_dir\n",
        "\n",
        "def clone_repo(url, dir, branch, output_widget):\n",
        "    \"\"\"Clones the specified repository.\"\"\"\n",
        "    try:\n",
        "        if not os.path.exists(dir):\n",
        "            !git clone -b {branch} {url} {dir}\n",
        "    except Exception as e:\n",
        "        with output_widget:\n",
        "            print(f\"Error cloning repo: {e}\")\n",
        "\n",
        "def install_dependencies(output_widget):\n",
        "    \"\"\"Installs the required dependencies.\"\"\"\n",
        "    try:\n",
        "        !apt install aria2\n",
        "        !pip install -q torch==2.1.0+cu121 diffusers[torch]==0.25.0 transformers==4.36.0 einops==0.6.0 open-clip-torch==2.20.0 invisible-watermark  jax[cuda12_pip]==0.4.23  -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "        !pip install -q xformers==0.0.23\n",
        "    except Exception as e:\n",
        "        with output_widget:\n",
        "            print(f\"Error installing dependencies: {e}\")\n",
        "\n",
        "def prepare_environment():\n",
        "    \"\"\"Sets environment variables.\"\"\"\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.9,max_split_size_mb:512\"\n",
        "    os.environ[\"TCMALLOC_AGGRESSIVE_DECOMMIT\"] = \"t\"\n",
        "    os.environ[\"CUDA_MODULE_LOADING\"] = \"LAZY\"\n",
        "\n",
        "\n",
        "def main_setup(b):\n",
        "    \"\"\"Main function for setting up the environment (linked to the setup button).\"\"\"\n",
        "    root_dir = root_dir_widget.value\n",
        "    repo_url = repo_url_widget.value\n",
        "    branch = branch_widget.value\n",
        "\n",
        "    setup_environment(root_dir, repo_url, branch, output_widget)\n",
        "\n",
        "\n",
        "# Link the button to the main setup function\n",
        "setup_button.on_click(main_setup)\n",
        "\n",
        "# Display the input widgets and setup button\n",
        "display(root_dir_widget)\n",
        "display(repo_url_widget)\n",
        "display(branch_widget)\n",
        "display(setup_button)\n",
        "display(output_widget)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### ♻ **Clean Folder**\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Function to clear and delete a folder\n",
        "def clear_and_delete_folder(colab_folder_path):\n",
        "    try:\n",
        "        # Use shutil.rmtree to remove all files and subdirectories\n",
        "        shutil.rmtree(colab_folder_path)\n",
        "        display(Markdown(f\"Deleted all contents in folder: `{colab_folder_path}`\"))\n",
        "    except Exception as e:\n",
        "        display(Markdown(f\"Error deleting folder `{colab_folder_path}`: {e}\"))\n",
        "\n",
        "# @markdown ### Folder Path for Deletion\n",
        "colab_folder_path = \"\" # @param {type: \"string\"}\n",
        "\n",
        "# Call the function to clear and delete the folder\n",
        "clear_and_delete_folder(colab_folder_path)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dD92f0lVd2Sh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### ♻ **Fix Before Converting (Optional, may not work)**\n",
        "#@markdown This MAY not work for SDXL, if you require key fixings and this works let me know!\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "from safetensors import safe_open\n",
        "from safetensors.torch import save_file\n",
        "import os\n",
        "\n",
        "#@markdown ### Model Type\n",
        "model_type = \"\" # @param [\"\", \"vae\"]\n",
        "\n",
        "#@markdown ### Input path\n",
        "load_path = \"\" # @param {type: \"string\"}\n",
        "\n",
        "#@markdown ### Output path\n",
        "save_path = \"\" # @param {type: \"string\"}\n",
        "\n",
        "\n",
        "def fix_diffusers_model_conversion(load_path, save_path, model_type):\n",
        "   try:\n",
        "\n",
        "    # load original\n",
        "    tensors = {}\n",
        "    with safe_open(load_path, framework=\"pt\") as f:\n",
        "        for key in f.keys():\n",
        "            tensors[key] = f.get_tensor(key)\n",
        "\n",
        "        # migrate\n",
        "        new_tensors = {}\n",
        "        for k, v in tensors.items():\n",
        "            new_key = k\n",
        "            # only fix the vae\n",
        "            if model_type == \"vae\" and 'first_stage_model.' in k:\n",
        "                # migrate q, k, v keys\n",
        "                new_key = new_key.replace('.to_q.weight', '.q.weight')\n",
        "                new_key = new_key.replace('.to_q.bias', '.q.bias')\n",
        "                new_key = new_key.replace('.to_k.weight', '.k.weight')\n",
        "                new_key = new_key.replace('.to_k.bias', '.k.bias')\n",
        "                new_key = new_key.replace('.to_v.weight', '.v.weight')\n",
        "                new_key = new_key.replace('.to_v.bias', '.v.bias')\n",
        "            new_tensors[new_key] = v\n",
        "\n",
        "        # save\n",
        "        save_file(new_tensors, save_path)\n",
        "        display(Markdown(f\"Keys have been fixed and the model saved to {save_path}\"))\n",
        "\n",
        "   except Exception as e:\n",
        "       display(Markdown(f\"An error occurred during key fixing: {e}\"))\n",
        "\n",
        "# Example usage\n",
        "fix_diffusers_model_conversion(load_path, save_path, model_type)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "olgK_uoDd26M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Download XL MODELS**\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1. You must specify a Hugging Face Token if the model you are downloading is in a private Hugging Face Repository.\n",
        "2. You must specify a URL to an SDXL model, which can be a Google Drive Link, a Hugging Face Model or direct URL to a file.\n",
        "3. You may optionally specify a folder where you would like to save the file. If you do not specify one, it will be stored in `/content/models` folder.\n",
        "4. Click the \"Download SDXL Model\" to begin the process.\n"
      ],
      "metadata": {
        "id": "2yvTwpwweOP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "import gdown\n",
        "import requests\n",
        "import subprocess\n",
        "from urllib.parse import urlparse, unquote\n",
        "from pathlib import Path\n",
        "from ipywidgets import Text, Button, Output\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# @markdown Place your Huggingface [Read Token](https://huggingface.co/settings/tokens) Here.\n",
        "HUGGINGFACE_TOKEN = \"\"#@param {type: \"string\"}\n",
        "\n",
        "# @markdown Place your SDXL Model URL Here.\n",
        "SDXL_MODEL_URL = \"\" #@param {type: \"string\"}\n",
        "\n",
        "#@markdown Specify the output path for the files here\n",
        "OUTPUT_PATH = \"/content/models\" #@param {type: \"string\"}\n",
        "\n",
        "# Create a button to trigger the download\n",
        "download_button = Button(description=\"Download SDXL Model\")\n",
        "\n",
        "# Create an output widget to display the results\n",
        "output_widget = Output()\n",
        "\n",
        "def download_sdxl_model(url, dst, huggingface_token, output_widget):\n",
        "    \"\"\"Downloads an SDXL model from the given URL.\"\"\"\n",
        "    def get_supported_extensions():\n",
        "        return tuple([\".ckpt\", \".safetensors\", \".pt\", \".pth\"])\n",
        "\n",
        "    def get_filename(url):\n",
        "        extensions = get_supported_extensions()\n",
        "\n",
        "        if url.endswith(tuple(extensions)):\n",
        "            filename = os.path.basename(url)\n",
        "        else:\n",
        "            response = requests.get(url, stream=True)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            if 'content-disposition' in response.headers:\n",
        "                content_disposition = response.headers['content-disposition']\n",
        "                filename = re.findall('filename=\"?([^\"]+)\"?', content_disposition)[0]\n",
        "            else:\n",
        "                url_path = urlparse(url).path\n",
        "                filename = unquote(os.path.basename(url_path))\n",
        "\n",
        "        if filename.endswith(tuple(get_supported_extensions())):\n",
        "            return filename\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def parse_args(config):\n",
        "        args = []\n",
        "\n",
        "        for k, v in config.items():\n",
        "            if k.startswith(\"_\"):\n",
        "                args.append(f\"{v}\")\n",
        "            elif isinstance(v, str) and v is not None:\n",
        "                args.append(f'--{k}={v}')\n",
        "            elif isinstance(v, bool) and v:\n",
        "                args.append(f\"--{k}\")\n",
        "            elif isinstance(v, float) and not isinstance(v, bool):\n",
        "                args.append(f\"--{k}={v}\")\n",
        "            elif isinstance(v, int) and not isinstance(v, bool):\n",
        "                args.append(f\"--{k}={v}\")\n",
        "\n",
        "        return args\n",
        "\n",
        "    def aria2_download(dir, filename, url, huggingface_token):\n",
        "        user_header = f\"Authorization: Bearer {huggingface_token}\"\n",
        "\n",
        "        aria2_config = {\n",
        "            \"console-log-level\"         : \"error\",\n",
        "            \"summary-interval\"          : 10,\n",
        "            \"header\"                    : user_header if \"huggingface.co\" in url else None,\n",
        "            \"continue\"                  : True,\n",
        "            \"max-connection-per-server\" : 16,\n",
        "            \"min-split-size\"            : \"1M\",\n",
        "            \"split\"                     : 16,\n",
        "            \"dir\"                       : dir,\n",
        "            \"out\"                       : filename,\n",
        "            \"_url\"                      : url,\n",
        "        }\n",
        "        aria2_args = parse_args(aria2_config)\n",
        "        subprocess.run([\"aria2c\", *aria2_args])\n",
        "\n",
        "    def gdown_download(url, dst, filepath):\n",
        "        if \"/uc?id/\" in url or \"/file/d/\" in url:\n",
        "            return gdown.download(url, filepath, quiet=False)\n",
        "        elif \"/drive/folders/\" in url:\n",
        "            os.chdir(dst)\n",
        "            return gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "\n",
        "    filename = get_filename(url)\n",
        "    filepath = os.path.join(dst, filename)\n",
        "\n",
        "    try:\n",
        "        if \"drive.google.com\" in url:\n",
        "            gdown = gdown_download(url, dst, filepath)\n",
        "        elif url.startswith(\"/content/drive/MyDrive/\"):\n",
        "            return url\n",
        "        else:\n",
        "            if \"huggingface.co\" in url:\n",
        "                if \"/blob/\" in url:\n",
        "                    url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "            aria2_download(dst, filename, url, huggingface_token)\n",
        "        with output_widget:\n",
        "            print(f\"Model downloaded to: {dst}\")\n",
        "    except Exception as e:\n",
        "        with output_widget:\n",
        "            print(f\"An error occurred during the download: {e}\")\n",
        "\n",
        "\n",
        "def main_download(b):\n",
        "    \"\"\"Main download function linked to the download button.\"\"\"\n",
        "    try:\n",
        "        download_sdxl_model(SDXL_MODEL_URL, OUTPUT_PATH, HUGGINGFACE_TOKEN, output_widget)\n",
        "    except Exception as e:\n",
        "        with output_widget:\n",
        "            print(f\"An error occurred in the main download function: {e}\")\n",
        "\n",
        "\n",
        "# Link the download button to the main download function\n",
        "download_button.on_click(main_download)\n",
        "\n",
        "# Display the download-related widgets\n",
        "display(download_button)\n",
        "display(output_widget)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jQD1DlnpeIy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# @title ## **Convert SDXL to Diffusers**\n",
        "import os\n",
        "import urllib.request\n",
        "from ipywidgets import Text, Button, Output\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "#@markdown ### **Conversion Config**\n",
        "model_to_load = \"\" #@param {'type': 'string'}\n",
        "script_url = \"https://raw.githubusercontent.com/Ktiseos-Nyx/SDXL_COLAB_DIFFUSERS_CONVERT/refs/heads/main/sdxl_to_diffusers_2025_colab.py\" #@param {'type': 'string'}\n",
        "global_step = 0 #@param {type:\"integer\"}\n",
        "epoch = 0 #@param {type:\"integer\"}\n",
        "save_precision_as = \"fp16\" #@param [\"fp16\",\"bf16\",\"float\"] {'allow-input': false}\n",
        "reference_model = \"stabilityai/stable-diffusion-xl-base-1.0\" #@param {'type': 'string'}\n",
        "output_path = \"/content/output\" #@param {type: \"string\"}\n",
        "\n",
        "# Create a button to trigger the conversion\n",
        "convert_button = Button(description=\"Convert SDXL to Diffusers\")\n",
        "output_widget = Output()\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "from diffusers import StableDiffusionXLPipeline\n",
        "#import library.sdxl_model_util as sdxl_model_util\n",
        "\n",
        "def convert_model(args):\n",
        "    load_dtype = torch.float16 if args.fp16 else None\n",
        "    save_dtype = get_save_dtype(args)\n",
        "\n",
        "    is_load_checkpoint = determine_load_checkpoint(args.model_to_load)\n",
        "    is_save_checkpoint = not is_load_checkpoint  # reverse of load model\n",
        "\n",
        "    loaded_model_data = load_sdxl_model(args, is_load_checkpoint, load_dtype)\n",
        "    convert_and_save_sdxl_model(args, is_save_checkpoint, loaded_model_data, save_dtype)\n",
        "\n",
        "def get_save_dtype(args):\n",
        "    if args.save_precision_as == \"fp16\":\n",
        "        return torch.float16\n",
        "    elif args.save_precision_as == \"bf16\":\n",
        "        return torch.bfloat16\n",
        "    elif args.save_precision_as == \"float\":\n",
        "        return torch.float\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def determine_load_checkpoint(model_to_load):\n",
        "    if model_to_load.endswith('.ckpt') or model_to_load.endswith('.safetensors'):\n",
        "        return True\n",
        "    elif os.path.isdir(model_to_load):\n",
        "        required_folders = {\"unet\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"scheduler\", \"vae\"}\n",
        "        if required_folders.issubset(set(os.listdir(model_to_load))) and os.path.isfile(os.path.join(model_to_load, \"model_index.json\")):\n",
        "            return False\n",
        "    return None  # handle this case as required\n",
        "\n",
        "def load_sdxl_model(args, is_load_checkpoint, load_dtype):\n",
        "    model_load_message = \"checkpoint\" if is_load_checkpoint else \"Diffusers\" + (\" as fp16\" if args.fp16 else \"\")\n",
        "    print(f\"Loading {model_load_message}: {args.model_to_load}\")\n",
        "\n",
        "    if is_load_checkpoint:\n",
        "        loaded_model_data = load_from_sdxl_checkpoint(args)\n",
        "    else:\n",
        "        loaded_model_data = load_sdxl_from_diffusers(args, load_dtype)\n",
        "\n",
        "    return loaded_model_data\n",
        "\n",
        "def load_from_sdxl_checkpoint(args):\n",
        "    #text_encoder1, text_encoder2, vae, unet, _, _ = sdxl_model_util.load_models_from_sdxl_checkpoint(\n",
        "    #    \"sdxl_base_v1-0\", args.model_to_load, \"cpu\"\n",
        "    #)\n",
        "\n",
        "    #Implement Load model from ckpt or safetensors\n",
        "\n",
        "    #Define these at the end.\n",
        "    text_encoder1, text_encoder2, vae, unet = None,None,None,None\n",
        "    print(\"Loading from Checkpoint not implemented, please implement based on your model needs\")\n",
        "    return text_encoder1, text_encoder2, vae, unet\n",
        "\n",
        "def load_sdxl_from_diffusers(args, load_dtype):\n",
        "    pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
        "        args.model_to_load, torch_dtype=load_dtype, tokenizer=None, tokenizer_2=None, scheduler=None\n",
        "        )\n",
        "    text_encoder1 = pipeline.text_encoder\n",
        "    text_encoder2 = pipeline.text_encoder_2\n",
        "    vae = pipeline.vae\n",
        "    unet = pipeline.unet\n",
        "\n",
        "    return text_encoder1, text_encoder2, vae, unet\n",
        "\n",
        "def convert_and_save_sdxl_model(args, is_save_checkpoint, loaded_model_data, save_dtype):\n",
        "    text_encoder1, text_encoder2, vae, unet = loaded_model_data\n",
        "    model_save_message = \"checkpoint\" + (\"\" if save_dtype is None else f\" in {save_dtype}\") if is_save_checkpoint else \"Diffusers\"\n",
        "    print(f\"Converting and saving as {model_save_message}: {args.model_to_save}\")\n",
        "\n",
        "    if is_save_checkpoint:\n",
        "        save_sdxl_as_checkpoint(args, text_encoder1, text_encoder2, vae, unet, save_dtype)\n",
        "    else:\n",
        "        save_sdxl_as_diffusers(args,  text_encoder1, text_encoder2, vae, unet, save_dtype)\n",
        "\n",
        "def save_sdxl_as_checkpoint(args, text_encoder1, text_encoder2, vae, unet, save_dtype):\n",
        "    logit_scale = None\n",
        "    ckpt_info = None\n",
        "\n",
        "    #key_count = sdxl_model_util.save_stable_diffusion_checkpoint(\n",
        "    #    args.model_to_save, text_encoder1, text_encoder2, unet, args.epoch, args.global_step, ckpt_info, vae, logit_scale, save_dtype\n",
        "    #    )\n",
        "\n",
        "    print(f\"Saving as Checkpoint not implemented, please implement based on your model needs\")\n",
        "    #print(f\"Model saved. Total converted state_dict keys: {key_count}\")\n",
        "\n",
        "def save_sdxl_as_diffusers(args, text_encoder1, text_encoder2, vae, unet, save_dtype):\n",
        "    reference_model_message = args.reference_model if args.reference_model is not None else 'default model'\n",
        "    print(f\"Copying scheduler/tokenizer config from: {reference_model_message}\")\n",
        "    #sdxl_model_util.save_diffusers_checkpoint(\n",
        "    #    args.model_to_save, text_encoder1, text_encoder2, unet, args.reference_model, vae, True, save_dtype\n",
        "    #)\n",
        "\n",
        "    # Save diffusers pipeline\n",
        "    pipeline = StableDiffusionXLPipeline(\n",
        "        vae=vae,\n",
        "        text_encoder=text_encoder1,\n",
        "        text_encoder_2=text_encoder2,\n",
        "        unet=unet,\n",
        "        scheduler=None, # Replace None if there is a scheduler\n",
        "        tokenizer=None, # Replace None if there is a tokenizer\n",
        "        tokenizer_2=None # Replace None if there is a tokenizer_2\n",
        "    )\n",
        "\n",
        "    pipeline.save_pretrained(args.model_to_save)\n",
        "    print(f\"Model saved as {save_dtype}.\")\n",
        "\n",
        "def get_save_path(args, is_save_checkpoint):\n",
        "    basename = os.path.splitext(args.model_to_load)[0]\n",
        "    if is_save_checkpoint:\n",
        "        return increment_filename(basename)\n",
        "    else:\n",
        "        return increment_filename(basename + \".safetensors\")\n",
        "\n",
        "def increment_filename(filename):\n",
        "    base, ext = os.path.splitext(filename)\n",
        "    counter = 1\n",
        "    while os.path.exists(filename):\n",
        "        filename = f\"{base}({counter}){ext}\"\n",
        "        counter += 1\n",
        "    return filename\n",
        "\n",
        "def setup_parser() -> argparse.ArgumentParser:\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--fp16\", action=\"store_true\", help=\"load as fp16 (Diffusers only)\")\n",
        "    parser.add_argument(\"--save_precision_as\", type=str, default=\"no\", choices=[\"fp16\", \"bf16\", \"float\"], help=\"save precision\")\n",
        "    parser.add_argument(\"--epoch\", type=int, default=0, help=\"epoch to write to checkpoint\")\n",
        "    parser.add_argument(\"--global_step\", type=int, default=0, help=\"global_step to write to checkpoint\")\n",
        "    parser.add_argument(\"--reference_model\", type=str, default=None, help=\"reference Diffusers model to copy scheduler/tokenizer config from, used when saving as Diffusers format, default is `runwayml/stable-diffusion-v1-5` or `stabilityai/stable-diffusion-2-1`\")\n",
        "    parser.add_argument(\"--model_to_load\", type=str, required=True, help=\"model to load: checkpoint file or Diffusers model's directory\")\n",
        "    return parser\n",
        "\n",
        "def main_convert(b):\n",
        "    #Manually configure the args to feed into convert model\n",
        "    class Args:\n",
        "        def __init__(self, model_to_load, save_precision_as, epoch, global_step, reference_model, output_path, fp16):\n",
        "            self.model_to_load = model_to_load\n",
        "            self.save_precision_as = save_precision_as\n",
        "            self.epoch = epoch\n",
        "            self.global_step = global_step\n",
        "            self.reference_model = reference_model\n",
        "            self.output_path = output_path\n",
        "            self.fp16 = fp16\n",
        "\n",
        "    args = Args(model_to_load, save_precision_as, epoch, global_step, reference_model, output_path, False)\n",
        "    args.model_to_save = get_save_path(args, determine_load_checkpoint(args.model_to_load))\n",
        "\n",
        "    convert_model(args)\n",
        "    with output_widget:\n",
        "        print(f\"Conversion complete to {output_path}!\")\n",
        "\n",
        "\n",
        "# Link the button to the main function\n",
        "convert_button.on_click(main_convert)\n",
        "\n",
        "# Display the widgets\n",
        "display(convert_button)\n",
        "display(output_widget)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "uSA8a1lCecqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inference**\n"
      ],
      "metadata": {
        "id": "3wHwHMKge5wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Run pipeline**"
      ],
      "metadata": {
        "id": "akl0skLke93L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import autocast\n",
        "from diffusers.models import AutoencoderKL\n",
        "from diffusers import StableDiffusionXLPipeline, EulerAncestralDiscreteScheduler\n",
        "\n",
        "model = \"/content/models/counterfeitxl_\"\n",
        "vae = AutoencoderKL.from_pretrained(\"stabilityai/sdxl-vae\")\n",
        "\n",
        "pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "    model,\n",
        "    torch_dtype=torch.float16,\n",
        "    use_safetensors=True,\n",
        "    variant=\"fp16\",\n",
        "    vae=vae\n",
        "    )\n",
        "pipe.scheduler = EulerAncestralDiscreteScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.to('cuda')"
      ],
      "metadata": {
        "id": "xUzFeqI4e2qf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Generate**"
      ],
      "metadata": {
        "id": "uSlVHGk0fC-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "prompt = \"masterpiece, best quality, 1girl, green hair, sweater, looking at viewer, upper body, beanie, outdoors, watercolor, night, turtleneck\"\n",
        "negative_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry\"\n",
        "output = \"/content/anime_girl.png\"\n",
        "\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    negative_prompt=negative_prompt,\n",
        "    width=1024,\n",
        "    height=1024,\n",
        "    guidance_scale=12,\n",
        "    target_size=(1024,1024),\n",
        "    original_size=(4096,4096),\n",
        "    num_inference_steps=50\n",
        "    ).images[0]\n",
        "\n",
        "image.save(output)\n",
        "image = Image.open(output)\n",
        "plt.imshow(image)\n",
        "plt.axis('off') # to hide the axis\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aXeC4Fg-fFkA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Clear Memory**"
      ],
      "metadata": {
        "id": "OfQoyV5gfKNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "\n",
        "if \"pipe\" in globals():\n",
        "    del pipe\n",
        "if \"image\" in globals():\n",
        "    del image\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XOzB2z6ufKrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deploy"
      ],
      "metadata": {
        "id": "oIJDfIaJfNje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### **Huggingface Hub config**\n",
        "from huggingface_hub import login, HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "# @markdown Login to Huggingface Hub\n",
        "# @markdown > Get **your** huggingface `WRITE` token [here](https://huggingface.co/settings/tokens)\n",
        "write_token = \"\"  # @param {type:\"string\"}\n",
        "# @markdown Fill this if you want to upload to your organization, or just leave it empty.\n",
        "orgs_name = \"\"  # @param{type:\"string\"}\n",
        "# @markdown If your model repo does not exist, it will automatically create it.\n",
        "model_name = \"\"  # @param {type:\"string\"}\n",
        "make_private = False  # @param{type:\"boolean\"}\n",
        "\n",
        "def authenticate(write_token):\n",
        "    login(write_token, add_to_git_credential=True)\n",
        "    api = HfApi()\n",
        "    return api.whoami(write_token), api\n",
        "\n",
        "def create_model_repo(api, user, orgs_name, model_name, make_private=False):\n",
        "    if orgs_name == \"\":\n",
        "        repo_id = user[\"name\"] + \"/\" + model_name.strip()\n",
        "    else:\n",
        "        repo_id = orgs_name + \"/\" + model_name.strip()\n",
        "\n",
        "    try:\n",
        "        validate_repo_id(repo_id)\n",
        "        api.create_repo(repo_id=repo_id, repo_type=\"model\", private=make_private)\n",
        "        print(f\"Model repo '{repo_id}' didn't exist, creating repo\")\n",
        "    except HfHubHTTPError as e:\n",
        "        print(f\"Model repo '{repo_id}' exists, skipping create repo\")\n",
        "\n",
        "    print(f\"Model repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
        "\n",
        "    return repo_id\n",
        "\n",
        "user, api = authenticate(write_token)\n",
        "\n",
        "if model_name:\n",
        "    model_repo = create_model_repo(api, user, orgs_name, model_name, make_private)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JnT7DK6IesR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### **Upload Model**\n",
        "# @title ### **Upload to Huggingface**\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# @markdown This will be uploaded to model repo\n",
        "model_path = \"/content/models/counterfeitxl_\"  # @param {type :\"string\"}\n",
        "path_in_repo = \"\"  # @param {type :\"string\"}\n",
        "project_name = os.path.basename(model_path)\n",
        "\n",
        "# @markdown Other Information\n",
        "commit_message = \"\"  # @param {type :\"string\"}\n",
        "\n",
        "def is_diffusers_model(model_path):\n",
        "    required_folders = {\"unet\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"scheduler\", \"vae\"}\n",
        "    return required_folders.issubset(set(os.listdir(model_path))) and os.path.isfile(os.path.join(model_path, \"model_index.json\"))\n",
        "\n",
        "def upload_model(model_paths, is_folder: bool):\n",
        "    path_obj = Path(model_paths)\n",
        "    trained_model = path_obj.parts[-1]\n",
        "\n",
        "    path_in_repo_local = path_in_repo if path_in_repo and not is_diffusers_model(model_paths) else \"\"\n",
        "\n",
        "    notification = f\"Uploading {trained_model} from {model_paths} to https://huggingface.co/{model_repo}\"\n",
        "    print(notification)\n",
        "\n",
        "    if is_folder:\n",
        "        if is_diffusers_model(model_paths):\n",
        "            commit_message = f\"Upload diffusers format: {trained_model}\"\n",
        "            print(\"Detected diffusers model. Adjusting upload parameters.\")\n",
        "        else:\n",
        "            commit_message = f\"Upload checkpoint: {trained_model}\"\n",
        "            print(\"Detected regular model. Adjusting upload parameters.\")\n",
        "\n",
        "        api.upload_folder(\n",
        "            folder_path=model_paths,\n",
        "            path_in_repo=path_in_repo_local,\n",
        "            repo_id=model_repo,\n",
        "            commit_message=commit_message,\n",
        "            ignore_patterns=\".ipynb_checkpoints\",\n",
        "        )\n",
        "    else:\n",
        "        commit_message = f\"Upload file: {trained_model}\"\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=model_paths,\n",
        "            path_in_repo=path_in_repo_local,\n",
        "            repo_id=model_repo,\n",
        "            commit_message=commit_message,\n",
        "        )\n",
        "\n",
        "    success_notification = f\"Upload successful. Check the model at https://huggingface.co/{model_repo}/tree/main\"\n",
        "    print(success_notification)\n",
        "\n",
        "def upload():\n",
        "    if model_path.endswith((\".ckpt\", \".safetensors\", \".pt\")):\n",
        "        upload_model(model_path, False)\n",
        "    else:\n",
        "        upload_model(model_path, True)\n",
        "\n",
        "upload()\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "r1n09nxZfXxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ">## Credits:\n",
        "\n",
        "\n",
        "| Patched Origin | Description | Link |\n",
        "| --- | --- | --- |\n",
        "|Patched from| ARCHIVED |[SDXL - Linaqruf](https://colab.research.google.com/github/Linaqruf/sdxl-model-converter/blob/main/sdxl_model_converter.ipynb)\n",
        "|***Linaqruf @ Github***: |https://github.com/Linaqruf\n",
        "|Linaqruf Ko-Fi | [![](https://dcbadge.vercel.app/api/shield/850007095775723532?style=flat)](https://lookup.guru/850007095775723532) [![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/linaqruf)\n",
        "| Linaqruf Saweria |<a href=\"https://saweria.co/linaqruf\"><img alt=\"Saweria\" src=\"https://img.shields.io/badge/Saweria-7B3F00?style=flat&logo=ko-fi&logoColor=white\"/></a>\n",
        "\n"
      ],
      "metadata": {
        "id": "lmTFYqWYgd_8"
      }
    }
  ]
}