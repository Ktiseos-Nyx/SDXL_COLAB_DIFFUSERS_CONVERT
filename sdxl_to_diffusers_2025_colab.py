# -*- coding: utf-8 -*-
"""sdxl_to_diffusers_2025_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CcSCmUB_UkT-8TlUkwDDKnHB4T7nti01

# **SDXL Model Converter Colab Edition**

> ## **âš ï¸ IMPORTANT: Google Colab AUP Warning & Xformers Disclaimer âš ï¸**
---
This Google Colab notebook is designed to convert Stable Diffusion XL (SDXL) model checkpoints to the `diffusers` format. It uses code from the `kohya-ss/sd-scripts` repository, which is primarily designed for training Stable Diffusion models and may cause the notebook to violate the Google Colab Acceptable Use Policy (AUP), even if you are not actively training a model.

**Therefore, using this notebook carries a significant risk of AUP violations and may lead to the suspension of your Google Colab account. Use at your own risk.**

Furthermore, the `xformers` library, which is installed as part of this notebook, may not be compatible with all hardware configurations or versions of PyTorch, and may cause errors or unexpected behavior. The reliability of this code and the functionality of `xformers` is entirely at your own risk.

**This Notebook is Provided "AS-IS" with No Warranty or Guarantee of Functionality or Support.**


---
> ## Collaboration

>I am NOT A programmer by nature, I patch with what little knowledge I have. I Failed programming several times over the years, so if something needs cleaning up and you want to patch it - pull request it!
---
>## About


>We are a system of over 300 alters, proudly navigating life with Dissociative Identity Disorder, ADHD, Autism, and CPTSD. We believe in the potential of AI to break down barriers and enhance aspects of mental health, even as it presents challenges. Our creative journey is an ongoing exploration of identity and expression, and we invite you to join us in this adventure.

---


>## Credits:


| Patched Origin | Description | Link |
| --- | --- | --- |
|Patched from| ARCHIVED |[SDXL - Linaqruf](https://colab.research.google.com/github/Linaqruf/sdxl-model-converter/blob/main/sdxl_model_converter.ipynb)
|***Linaqruf @ Github***: |https://github.com/Linaqruf
|Linaqruf Ko-Fi | [![](https://dcbadge.vercel.app/api/shield/850007095775723532?style=flat)](https://lookup.guru/850007095775723532) [![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/linaqruf)
| Linaqruf Saweria |<a href="https://saweria.co/linaqruf"><img alt="Saweria" src="https://img.shields.io/badge/Saweria-7B3F00?style=flat&logo=ko-fi&logoColor=white"/></a>
>## Social Media
---
| Link Name| Description | Link |
| --- | --- | --- |
| [Huggingface Backup](https://colab.research.google.com/github/kieranxsomer/HuggingFace_Backup/blob/main/HuggingFace_Backup.ipynb) | backup checkpoints! | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/kieranxsomer/HuggingFace_Backup/blob/main/HuggingFace_Backup.ipynb)
|Discord| E&D Discord |[Invite](https://discord.gg/5t2kYxt7An)
|CivitAi| Duskfallcrew @ Civitai |[Duskfallcrew](https://civitai.com/user/duskfallcrew/)
|Huggingface| E&D Huggingface |[Earth & Dusk](https://huggingface.co/EarthnDusk)
|Ko-Fi| Kofi Support |[![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/Z8Z8L4EO)
|Github| Duskfallcrew Github |[Duskfallcrew](https://github.com/duskfallcrew)
| Youtube: | Duskfall Music|[Duskfall Music & More](https://www.youtube.com/channel/UCk7MGP7nrJz5awBSP75xmVw)
| Spotify: | E&D Royalty Free| [PLAYLIST](https://open.spotify.com/playlist/00R8x00YktB4u541imdSSf?si=57a8f0f0fe87434e)
|DA Group | AI Group| [DeviantArt Group](https://www.deviantart.com/diffusionai)
| Reddit | Earth & Dusk| [Subreddit](https://www.reddit.com/r/earthndusk/)

---

> ## **Setup Instructions:**

1.  **Google Colab Environment:** This notebook is designed to run on Google Colab with a GPU runtime enabled.
2.  **Root Directory:** The `root_dir` variable specifies where files will be stored. The default is `/content`. You can specify a different location by changing the value in the corresponding text field.
3. **Repository URL:** If you want to use a specific version of the `kohya-ss/sd-scripts` repository, then you can specify the URL using the text field for "Repo URL".
4. **Branch Name:** If you want to use a specific branch of the `kohya-ss/sd-scripts` repository, then you can specify the branch name using the text field for "Branch".
5. **Xformers URL:** You are using the official xformers library, however, if you have a custom built `xformers` wheel file, then you can specify the URL using the "Xformers URL" field. (Not Recommended).
6.  **Run Setup:** After providing the values, click the "Setup Environment" button to install all required dependencies and clone the necessary files, and prepare the environment variables.
7.  **Follow the Steps:** You are now able to continue with the following steps.

---

> ## **Important Note:**


Before diving in, ensure you create a Hugging Face token with write permissions. Follow this link for instructions on token creation.

You need to create a huggingface token, go to [this link](https://huggingface.co/settings/tokens), then `create new token` or copy available token with the `Write` role.

-----------------
"""

#@title ## **Install Kohya Script**
import os
from subprocess import getoutput
from ipywidgets import Text, Output, Button
from IPython.display import display, Markdown

# Create text widgets for input
root_dir_widget = Text(value="/content", placeholder="Root Directory", description="Root Directory:")
repo_url_widget = Text(value="https://github.com/kohya-ss/sd-scripts", placeholder="Repo URL", description="Repo URL:")
branch_widget = Text(value="sdxl", placeholder="Branch", description="Branch:")
output_widget = Output()
# Create a button to trigger the setup
setup_button = Button(description="Setup Environment")

# Directories
def setup_directories(root_dir):
    repo_dir = os.path.join(root_dir, "kohya-trainer")
    models_dir = os.path.join(root_dir, "models")
    tools_dir = os.path.join(repo_dir, "tools")
    vae_dir = os.path.join(root_dir, "vae")

    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(vae_dir, exist_ok=True)
    os.makedirs(tools_dir, exist_ok=True)
    os.makedirs(repo_dir, exist_ok=True)
    return repo_dir, models_dir, tools_dir, vae_dir


def clone_repo(url, dir, branch, output_widget):
    try:
        if not os.path.exists(dir):
            !git clone -b {branch} {url} {dir}
    except Exception as e:
        with output_widget:
           print(f"Error cloning repo: {e}")

def install_dependencies(output_widget):
    try:
        !apt install aria2
        !pip install -q torch==2.1.0+cu121 diffusers[torch]==0.25.0 transformers==4.36.0 einops==0.6.0 open-clip-torch==2.20.0 invisible-watermark  jax[cuda12_pip]==0.4.23  -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
        !pip install -q xformers==0.0.23
    except Exception as e:
        with output_widget:
            print(f"Error installing dependencies: {e}")

def prepare_environment():
    os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
    os.environ["SAFETENSORS_FAST_GPU"] = "1"
    os.environ["PYTHONWARNINGS"] = "ignore"
    os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "garbage_collection_threshold:0.9,max_split_size_mb:512"
    os.environ["TCMALLOC_AGGRESSIVE_DECOMMIT"] = "t"
    os.environ["CUDA_MODULE_LOADING"] = "LAZY"

def main(b):
    root_dir = root_dir_widget.value
    repo_url = repo_url_widget.value
    branch = branch_widget.value
    try:
        repo_dir, models_dir, tools_dir, vae_dir = setup_directories(root_dir)
        clone_repo(repo_url, repo_dir, branch, output_widget)
        install_dependencies(output_widget)
        prepare_environment()
        with output_widget:
            print(f"Root directory set to: {root_dir}")
            print("Setup Complete")
    except Exception as e:
         with output_widget:
             print(f"An error occurred: {e}")


# Link the button to the main function
setup_button.on_click(main)

display(Markdown("""
 **âš ï¸ WARNING:**
This Google Colab notebook includes tools from the `kohya-ss/sd-scripts` repository, which is **primarily designed for training Stable Diffusion models**. Using this code may cause your Google Colab account to be flagged for violating the Google Colab Acceptable Use Policy (AUP), **even if you are not actively training a model**.
"""))
display(Markdown("""
 **âš ï¸ IMPORTANT AUP RISK:**
 Using this notebook carries a **significant risk of AUP violations** which may lead to the suspension of your Google Colab account and it is provided **AS-IS** with no warranty or guarantee of support. **Use at your own risk.**
 """))
display(Markdown("""
 **âš ï¸ XFORMERS WARNING:**
 The `xformers` library may not be compatible with all hardware configurations or versions of PyTorch, and it may lead to errors. The use of `xformers` and the reliability of this code is entirely at your own risk.
 """))
display(root_dir_widget)
display(repo_url_widget)
display(branch_widget)
display(setup_button)
display(output_widget)

# @title ### â™» **Clean Folder**

from IPython.display import display, Markdown
import os
import shutil

# Function to clear and delete a folder
def clear_and_delete_folder(colab_folder_path):
    try:
        # Use shutil.rmtree to remove all files and subdirectories
        shutil.rmtree(colab_folder_path)
        display(Markdown(f"Deleted all contents in folder: `{colab_folder_path}`"))
    except Exception as e:
        display(Markdown(f"Error deleting folder `{colab_folder_path}`: {e}"))

# @markdown ### Folder Path for Deletion

colab_folder_path = "" # @param {type: "string"}

# Call the function to clear and delete the folder
clear_and_delete_folder(colab_folder_path)

# @title ### â™» **Fix Before Converting (Optional, may not work)**
#@markdown This MAY not work for SDXL, if you require key fixings and this works let me know!

from IPython.display import display, Markdown
from safetensors import safe_open
from safetensors.torch import save_file
import os

display(Markdown("""
 **âš ï¸ WARNING:**
 This Google Colab notebook includes tools from the `kohya-ss/sd-scripts` repository. This code is primarily designed for training Stable Diffusion models and it may cause the notebook to violate the Google Colab Acceptable Use Policy (AUP) even if you are not actively training a model. The use of `kohya-ss` may flag your account and you may be unable to run this notebook.
"""))
display(Markdown("""
 **âš ï¸ IMPORTANT:**
 Using this notebook carries a significant risk of AUP violations and may lead to the suspension of your account. Use at your own risk.
 """))
display(Markdown("""
 **âš ï¸ XFORMERS WARNING:**
 The `xformers` library may not be compatible with all hardware configurations or versions of PyTorch, and it may lead to errors. The use of `xformers` and the reliability of this code is entirely at your own risk.
 """))

#@markdown ### Model Type
model_type = "" # @param ["", "vae"]

#@markdown ### Input path
load_path = "" # @param {type: "string"}

#@markdown ### Output path
save_path = "" # @param {type: "string"}


def fix_diffusers_model_conversion(load_path, save_path, model_type):
   try:

    # load original
    tensors = {}
    with safe_open(load_path, framework="pt") as f:
        for key in f.keys():
            tensors[key] = f.get_tensor(key)

        # migrate
        new_tensors = {}
        for k, v in tensors.items():
            new_key = k
            # only fix the vae
            if model_type == "vae" and 'first_stage_model.' in k:
                # migrate q, k, v keys
                new_key = new_key.replace('.to_q.weight', '.q.weight')
                new_key = new_key.replace('.to_q.bias', '.q.bias')
                new_key = new_key.replace('.to_k.weight', '.k.weight')
                new_key = new_key.replace('.to_k.bias', '.k.bias')
                new_key = new_key.replace('.to_v.weight', '.v.weight')
                new_key = new_key.replace('.to_v.bias', '.v.bias')
            new_tensors[new_key] = v

        # save
        save_file(new_tensors, save_path)
        display(Markdown(f"Keys have been fixed and the model saved to {save_path}"))

   except Exception as e:
       display(Markdown(f"An error occurred during key fixing: {e}"))

# Example usage
fix_diffusers_model_conversion(load_path, save_path, model_type)

"""## **Download XL MODELS**

**âš ï¸ WARNING:**
THIS CELL ALLOWS YOU TO DOWNLOAD STABLE DIFFUSION XL MODELS INTO YOUR GOOGLE COLABORATORY NOTEBOOK. PLEASE BE AWARE THIS MAY BE OF VIOLATION OF YOUR AUP.

**âš ï¸ IMPORTANT:**
Using this notebook carries a significant risk of AUP violations and may lead to the suspension of your account. Use at your own risk.


---

**Instructions:**

1. You must specify a Hugging Face Token if the model you are downloading is in a private Hugging Face Repository.
2. You must specify a URL to an SDXL model, which can be a Google Drive Link, a Hugging Face Model or direct URL to a file.
3. You may optionally specify a folder where you would like to save the file. If you do not specify one, it will be stored in `/content/models` folder.
4. Click the "Download SDXL Model" to begin the process.
"""

# @title ## **Download SDXL**
import os
import re
import json
import glob
import gdown
import requests
import subprocess
from urllib.parse import urlparse, unquote
from pathlib import Path
from ipywidgets import Text, Button, Output
from IPython.display import display, Markdown

display(Markdown("""
 **âš ï¸ WARNING:**
 This Google Colab notebook includes tools from the `kohya-ss/sd-scripts` repository. This code is primarily designed for training Stable Diffusion models and it may cause the notebook to violate the Google Colab Acceptable Use Policy (AUP) even if you are not actively training a model. The use of `kohya-ss` may flag your account and you may be unable to run this notebook.
"""))
display(Markdown("""
 **âš ï¸ IMPORTANT:**
 Using this notebook carries a significant risk of AUP violations and may lead to the suspension of your account. Use at your own risk.
 """))
display(Markdown("""
 **âš ï¸ XFORMERS WARNING:**
 The `xformers` library may not be compatible with all hardware configurations or versions of PyTorch, and it may lead to errors. The use of `xformers` and the reliability of this code is entirely at your own risk.
 """))


# @markdown Place your Huggingface [Read Token](https://huggingface.co/settings/tokens) Here.
HUGGINGFACE_TOKEN = ""#@param {type: "string"}

# @markdown Place your SDXL Model URL Here.
SDXL_MODEL_URL = "" #@param {type: "string"}

#@markdown Specify the output path for the files here
OUTPUT_PATH = "/content/models" #@param {type: "string"}

# Create a button to trigger the download
download_button = Button(description="Download SDXL Model")

# Create an output widget to display the results
output_widget = Output()


def get_supported_extensions():
    return tuple([".ckpt", ".safetensors", ".pt", ".pth"])

def get_filename(url):
    extensions = get_supported_extensions()

    if url.endswith(tuple(extensions)):
        filename = os.path.basename(url)
    else:
        response = requests.get(url, stream=True)
        response.raise_for_status()

        if 'content-disposition' in response.headers:
            content_disposition = response.headers['content-disposition']
            filename = re.findall('filename="?([^"]+)"?', content_disposition)[0]
        else:
            url_path = urlparse(url).path
            filename = unquote(os.path.basename(url_path))

    if filename.endswith(tuple(get_supported_extensions())):
        return filename
    else:
        return None

def parse_args(config):
    args = []

    for k, v in config.items():
        if k.startswith("_"):
            args.append(f"{v}")
        elif isinstance(v, str) and v is not None:
            args.append(f'--{k}={v}')
        elif isinstance(v, bool) and v:
            args.append(f"--{k}")
        elif isinstance(v, float) and not isinstance(v, bool):
            args.append(f"--{k}={v}")
        elif isinstance(v, int) and not isinstance(v, bool):
            args.append(f"--{k}={v}")

    return args

def aria2_download(dir, filename, url, huggingface_token):
    user_header = f"Authorization: Bearer {huggingface_token}"

    aria2_config = {
        "console-log-level"         : "error",
        "summary-interval"          : 10,
        "header"                    : user_header if "huggingface.co" in url else None,
        "continue"                  : True,
        "max-connection-per-server" : 16,
        "min-split-size"            : "1M",
        "split"                     : 16,
        "dir"                       : dir,
        "out"                       : filename,
        "_url"                      : url,
    }
    aria2_args = parse_args(aria2_config)
    subprocess.run(["aria2c", *aria2_args])

def gdown_download(url, dst, filepath):
    if "/uc?id/" in url or "/file/d/" in url:
        return gdown.download(url, filepath, quiet=False)
    elif "/drive/folders/" in url:
        os.chdir(dst)
        return gdown.download_folder(url, quiet=True, use_cookies=False)

def download(url, dst, huggingface_token, output_widget):
    filename = get_filename(url)
    filepath = os.path.join(dst, filename)

    try:
        if "drive.google.com" in url:
            gdown = gdown_download(url, dst, filepath)
        elif url.startswith("/content/drive/MyDrive/"):
            return url
        else:
            if "huggingface.co" in url:
                if "/blob/" in url:
                    url = url.replace("/blob/", "/resolve/")
            aria2_download(dst, filename, url, huggingface_token)
        with output_widget:
            print(f"Model downloaded to: {dst}")
    except Exception as e:
       with output_widget:
            print(f"An error occurred during the download: {e}")


def main(b):
    try:
      download(SDXL_MODEL_URL, OUTPUT_PATH, HUGGINGFACE_TOKEN, output_widget)
    except Exception as e:
      with output_widget:
        print(f"An error occurred in the main function: {e}")


# Link the button to the main function
download_button.on_click(main)


display(download_button)
display(output_widget)

"""## **CONVERT SDXL MODELS**

**âš ï¸ WARNING:**
THIS CELL ALLOWS YOU TO CONVERT STABLE DIFFUSION MODELS, WHICH CAN BE CONSIDERED NSFW OR SIGNIFICANT RISK TO AUP VIOLATIONS and may lead to the suspension of your account. Use at your own risk.

**âš ï¸ IMPORTANT:**
Using this notebook carries a significant risk of AUP violations and may lead to the suspension of your account. Use at your own risk.


---
"""

# @title ## **Convert SDXL to Diffusers**
import os
import urllib.request
from ipywidgets import Text, Button, Output
from IPython.display import display, Markdown

display(Markdown("""
 **âš ï¸ WARNING:**
 This Google Colab notebook includes tools from the `kohya-ss/sd-scripts` repository. This code is primarily designed for training Stable Diffusion models and it may cause the notebook to violate the Google Colab Acceptable Use Policy (AUP) even if you are not actively training a model. The use of `kohya-ss` may flag your account and you may be unable to run this notebook.
"""))
display(Markdown("""
 **âš ï¸ IMPORTANT:**
 Using this notebook carries a significant risk of AUP violations and may lead to the suspension of your account. Use at your own risk.
 """))
display(Markdown("""
 **âš ï¸ XFORMERS WARNING:**
 The `xformers` library may not be compatible with all hardware configurations or versions of PyTorch, and it may lead to errors. The use of `xformers` and the reliability of this code is entirely at your own risk.
 """))
#@markdown ### **Conversion Config**
model_to_load = "" #@param {'type': 'string'}
script_url = "https://raw.githubusercontent.com/duskfallcrew/sdxl-model-converter/main/convert_sdxl_to_diffusers.py" #@param {'type': 'string'}
global_step = 0 #@param {type:"integer"}
epoch = 0 #@param {type:"integer"}
save_precision_as = "fp16" #@param ["fp16","bf16","float"] {'allow-input': false}
reference_model = "stabilityai/stable-diffusion-xl-base-1.0" #@param {'type': 'string'}
output_path = "/content/output" #@param {type: "string"}

# Create a button to trigger the conversion
convert_button = Button(description="Convert SDXL to Diffusers")
output_widget = Output()

def convert_dict(config):
    args = ""
    for k, v in config.items():
        if k.startswith("_"):
            args += f'"{v}" '
        elif isinstance(v, str):
            args += f'--{k}="{v}" '
        elif isinstance(v, bool) and v:
            args += f"--{k} "
        elif isinstance(v, float) and not isinstance(v, bool):
            args += f"--{k}={v} "
        elif isinstance(v, int) and not isinstance(v, bool):
            args += f"--{k}={v} "
    return args

def check_and_download_script(script_name, script_url):
    try:
        if not os.path.exists(script_name):
            with output_widget:
                print(f"{script_name} not found, downloading...")
            urllib.request.urlretrieve(script_url, script_name)
    except Exception as e:
      with output_widget:
          print(f"An error occurred during downloading: {e}")
          return False
    return True


def run_script(script_name, script_args, output_widget):
  try:
      !python {script_name} {script_args}
      with output_widget:
        print(f"Conversion complete to {output_path}!")
  except Exception as e:
    with output_widget:
      print(f"An error occurred during conversion: {e}")


def main(b):
    if not check_and_download_script("convert_sdxl_to_diffusers.py", script_url):
        return

    config = {
        "model_to_load": model_to_load,
        "save_precision_as": save_precision_as,
        "epoch": epoch,
        "global_step": global_step,
        "reference_model": reference_model,
        "output_path": output_path
    }
    run_script("convert_sdxl_to_diffusers.py", convert_dict(config), output_widget)

# Link the button to the main function
convert_button.on_click(main)

# Display the widgets
display(convert_button)
display(output_widget)

"""## **Deploy**"""

# @title ### **Huggingface Hub config**
from huggingface_hub import login, HfApi
from huggingface_hub.utils import validate_repo_id, HfHubHTTPError

# @markdown Login to Huggingface Hub
# @markdown > Get **your** huggingface `WRITE` token [here](https://huggingface.co/settings/tokens)
write_token = ""  # @param {type:"string"}
# @markdown Fill this if you want to upload to your organization, or just leave it empty.
orgs_name = ""  # @param{type:"string"}
# @markdown If your model repo does not exist, it will automatically create it.
model_name = ""  # @param {type:"string"}
make_private = False  # @param{type:"boolean"}

def authenticate(write_token):
    login(write_token, add_to_git_credential=True)
    api = HfApi()
    return api.whoami(write_token), api

def create_model_repo(api, user, orgs_name, model_name, make_private=False):
    if orgs_name == "":
        repo_id = user["name"] + "/" + model_name.strip()
    else:
        repo_id = orgs_name + "/" + model_name.strip()

    try:
        validate_repo_id(repo_id)
        api.create_repo(repo_id=repo_id, repo_type="model", private=make_private)
        print(f"Model repo '{repo_id}' didn't exist, creating repo")
    except HfHubHTTPError as e:
        print(f"Model repo '{repo_id}' exists, skipping create repo")

    print(f"Model repo '{repo_id}' link: https://huggingface.co/{repo_id}\n")

    return repo_id

user, api = authenticate(write_token)

if model_name:
    model_repo = create_model_repo(api, user, orgs_name, model_name, make_private)

# @title ### â™» **Upload to Huggingface**
from huggingface_hub import HfApi
from pathlib import Path
import os

api = HfApi()

# @markdown This will be uploaded to model repo
model_path = ""  # @param {type :"string"}
path_in_repo = ""  # @param {type :"string"}
project_name = os.path.basename(model_path)

# @markdown Other Information
commit_message = "Upload with ðŸš€ðŸ¤— E&D SDXL Diffusers notebook"  # @param {type :"string"}

def is_diffusers_model(model_path):
    required_folders = {"unet", "text_encoder", "text_encoder_2", "tokenizer", "tokenizer_2", "scheduler", "vae"}
    return required_folders.issubset(set(os.listdir(model_path))) and os.path.isfile(os.path.join(model_path, "model_index.json"))

def upload_model(model_paths, is_folder: bool):
    path_obj = Path(model_paths)
    trained_model = path_obj.parts[-1]

    path_in_repo_local = path_in_repo if path_in_repo and not is_diffusers_model(model_paths) else ""

    notification = f"Uploading {trained_model} from {model_paths} to https://huggingface.co/{model_repo}"
    print(notification)

    if is_folder:
        if is_diffusers_model(model_paths):
            commit_message = f"Upload diffusers format: {trained_model}"
            print("Detected diffusers model. Adjusting upload parameters.")
        else:
            commit_message = f"Upload checkpoint: {trained_model}"
            print("Detected regular model. Adjusting upload parameters.")

        api.upload_folder(
            folder_path=model_paths,
            path_in_repo=path_in_repo_local,
            repo_id=model_repo,
            commit_message=commit_message,
            ignore_patterns=".ipynb_checkpoints",
        )
    else:
        commit_message = f"Upload file: {trained_model}"
        api.upload_file(
            path_or_fileobj=model_paths,
            path_in_repo=path_in_repo_local,
            repo_id=model_repo,
            commit_message=commit_message,
        )

    success_notification = f"â™» Model upload complete, care to try again? Thanks for flying Stable Diffusion Airlines, you owe me five dollars.. Check the model at https://huggingface.co/{model_repo}/tree/main"
    print(success_notification)

def upload():
    if model_path.endswith((".ckpt", ".safetensors", ".pt")):
        upload_model(model_path, False)
    else:
        upload_model(model_path, True)

upload()